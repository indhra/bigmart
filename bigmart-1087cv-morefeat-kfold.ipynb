{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12978152,"sourceType":"datasetVersion","datasetId":8172119}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:38:04.649191Z","iopub.execute_input":"2025-09-07T13:38:04.649481Z","iopub.status.idle":"2025-09-07T13:38:07.900546Z","shell.execute_reply.started":"2025-09-07T13:38:04.649461Z","shell.execute_reply":"2025-09-07T13:38:07.899486Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor, HistGradientBoostingRegressor\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# --- 1. Data Loading and Preprocessing ---\n\ndef load_data(train_file, test_file):\n    \"\"\"\n    Loads and combines the training and testing datasets for consistent preprocessing.\n    \"\"\"\n    try:\n        train_df = pd.read_csv(train_file)\n        test_df = pd.read_csv(test_file)\n    except FileNotFoundError as e:\n        print(f\"Error: {e}. Please ensure the files exist.\")\n        return None, None\n    \n    train_df['source'] = 'train'\n    test_df['source'] = 'test'\n    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n    \n    # Store the original Item_Identifier and Outlet_Identifier for the submission file\n    test_ids = test_df[['Item_Identifier', 'Outlet_Identifier']]\n    return combined_df, test_ids\n\ndef preprocess_features(df):\n    \"\"\"\n    Performs advanced feature engineering, cleaning, and imputation.\n    \"\"\"\n    # Impute missing Item_Weight using the median per Item_Identifier\n    # Using median is more robust to outliers than mean.\n    df['Item_Weight'] = df.groupby('Item_Identifier')['Item_Weight'].transform(\n        lambda x: x.fillna(x.median())\n    )\n    \n    # Impute missing Outlet_Size based on the mode of the Outlet_Type\n    outlet_sizes_mode = df.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode().iloc[0])\n    df['Outlet_Size'] = df.apply(\n        lambda row: outlet_sizes_mode[row['Outlet_Type']] if pd.isna(row['Outlet_Size']) else row['Outlet_Size'],\n        axis=1\n    )\n    \n    # Handle categorical feature inconsistencies\n    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'})\n    \n    # --- Advanced Feature Engineering ---\n    \n    # 1. New feature for Outlet Age\n    df['Outlet_Years'] = 2013 - df['Outlet_Establishment_Year']\n    \n    # 2. Correcting and creating new visibility features\n    # First, replace 0 visibility with the mean visibility of that item across all outlets\n    visibility_means = df.groupby('Item_Identifier')['Item_Visibility'].transform(\n        lambda x: x.replace(0, x[x != 0].mean())\n    )\n    # If an item had 0 visibility across all its appearances, fill with the global average visibility\n    df['Item_Visibility'] = visibility_means.fillna(df['Item_Visibility'].mean())\n    \n    # Create a new feature: visibility per outlet type\n    df['Item_Visibility_per_Outlet_Type'] = df['Item_Visibility'] / df.groupby('Outlet_Type')['Item_Visibility'].transform('mean')\n\n    # 3. Item_MRP Bins\n    # Binning the Item_MRP into categories can help the model capture non-linear relationships.\n    df['Item_MRP_Bins'] = pd.cut(df['Item_MRP'], bins=[0, 70, 140, 210, 280], labels=['Low', 'Medium', 'High', 'Very_High'])\n    \n    # 4. Item_Type Grouping\n    df['Item_Type_Combined'] = df['Item_Identifier'].apply(lambda x: x[:2])\n    df['Item_Type_Combined'] = df['Item_Type_Combined'].replace({'DR': 'Drinks', 'FD': 'Food', 'NC': 'Non-Consumables'})\n    \n    return df\n\ndef separate_and_encode(df):\n    \"\"\"\n    Separates the combined data and applies a single OneHotEncoder.\n    \"\"\"\n    # Define features to be dropped before encoding\n    drop_cols = ['Item_Identifier', 'Outlet_Establishment_Year']\n    \n    # Separate data back into train and test sets\n    train_processed = df[df['source'] == 'train'].drop('source', axis=1)\n    test_processed = df[df['source'] == 'test'].drop('source', axis=1)\n\n    X_train = train_processed.drop(['Item_Outlet_Sales'] + drop_cols, axis=1)\n    y_train = train_processed['Item_Outlet_Sales']\n    \n    X_test = test_processed.drop(drop_cols, axis=1)\n\n    # Use OneHotEncoder within a ColumnTransformer\n    categorical_cols = [\n        'Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size',\n        'Outlet_Location_Type', 'Outlet_Type', 'Item_Type_Combined', 'Item_MRP_Bins'\n    ]\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n        ],\n        remainder='passthrough'\n    )\n\n    X_train_encoded = preprocessor.fit_transform(X_train)\n    X_test_encoded = preprocessor.transform(X_test)\n    \n    return X_train_encoded, y_train, X_test_encoded\n\n# --- 2. Ensemble Model Building and Training ---\n\ndef build_stacked_model():\n    \"\"\"\n    Builds a Stacking Regressor model with a more diverse set of base estimators.\n    \"\"\"\n    # Optimized hyperparameters for the base models to reduce overfitting\n    # Added 'tree_method' for XGBoost and 'device' for LightGBM to enable GPU usage.\n    estimators = [\n        ('xgb', xgb.XGBRegressor(\n            objective='reg:squarederror', n_estimators=100, learning_rate=0.02,\n            max_depth=2, subsample=0.7, colsample_bytree=0.7, reg_alpha=0.005, \n            random_state=42, n_jobs=-1, device='cuda'\n        )),\n        ('gbr', GradientBoostingRegressor(\n            n_estimators=500, learning_rate=0.02, max_depth=2,\n            min_samples_leaf=10, max_features='sqrt', random_state=42\n        )),\n        ('lgbm', lgb.LGBMRegressor(\n            objective='regression', n_estimators=500, learning_rate=0.03,\n            num_leaves=31, min_child_samples=20, subsample=0.7,  \n            colsample_bytree=0.7, reg_alpha=0.001, random_state=42, n_jobs=-1, silent=True,\n            device='gpu' # Enables GPU for faster training\n        )),\n        ('hgbm', HistGradientBoostingRegressor(\n            max_iter=500, learning_rate=0.03, max_leaf_nodes=31, \n            min_samples_leaf=20, random_state=42\n        ))\n    ]\n\n    # Use a more flexible meta-regressor like LassoCV\n    # The 'normalize' parameter was removed in recent scikit-learn versions, so it's been removed here.\n    meta_regressor = LassoCV(\n        eps=1e-5, n_alphas=50, random_state=42\n    )\n    \n    # Create the StackingRegressor with 10-fold cross-validation\n    stacked_model = StackingRegressor(\n        estimators=estimators,\n        final_estimator=meta_regressor,\n        cv=10, # Increased folds for more robust cross-validation\n        n_jobs=-1\n    )\n    \n    return stacked_model\n\n# --- 3. Cross-Validation and Prediction ---\n\ndef train_and_predict(model, X_train, y_train, X_test):\n    \"\"\"\n    Trains the model using K-Fold cross-validation and makes predictions.\n    \"\"\"\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_predictions = np.zeros(X_train.shape[0])\n    test_predictions = np.zeros(X_test.shape[0])\n    \n    print(\"Starting K-Fold Cross-Validation...\")\n    for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold+1}/{kf.n_splits} ---\")\n        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        model.fit(X_train_fold, y_train_fold)\n        \n        oof_preds = model.predict(X_val_fold)\n        oof_predictions[val_index] = oof_preds\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, oof_preds))\n        print(f\"Fold {fold+1} RMSE: {fold_rmse:.4f}\")\n        \n        test_predictions += model.predict(X_test) / kf.n_splits\n    \n    overall_rmse = np.sqrt(mean_squared_error(y_train, oof_predictions))\n    print(f\"\\nOverall Cross-Validation RMSE: {overall_rmse:.4f}\")\n    \n    return test_predictions\n\n# --- 4. Main Execution Block ---\n\ndef main():\n    \"\"\"\n    Main function to run the entire prediction pipeline.\n    \"\"\"\n    print(\"Loading and preprocessing data...\")\n    combined_df, test_ids = load_data('/kaggle/input/big-mart/train.csv', '/kaggle/input/big-mart/test.csv')\n\n    if combined_df is None:\n        return\n\n    combined_df = preprocess_features(combined_df.copy())\n    \n    X_train_encoded, y_train, X_test_encoded = separate_and_encode(combined_df)\n\n    print(\"Building the advanced ensemble model...\")\n    stacked_model = build_stacked_model()\n    \n    test_predictions = train_and_predict(stacked_model, X_train_encoded, y_train, X_test_encoded)\n    \n    # Create the submission DataFrame\n    submission_df = test_ids.copy()\n    submission_df['Item_Outlet_Sales'] = test_predictions\n    \n    # Post-process predictions to be non-negative\n    submission_df['Item_Outlet_Sales'] = np.maximum(0, submission_df['Item_Outlet_Sales'])\n    \n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nPrediction complete. The output file 'submission.csv' has been generated.\")\n    print(\"\\nHead of the submission file:\")\n    print(submission_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:44:55.049642Z","iopub.execute_input":"2025-09-07T13:44:55.049999Z","iopub.status.idle":"2025-09-07T13:49:34.704250Z","shell.execute_reply.started":"2025-09-07T13:44:55.049974Z","shell.execute_reply":"2025-09-07T13:49:34.703329Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Loading and preprocessing data...\nBuilding the advanced ensemble model...\nStarting K-Fold Cross-Validation...\n--- Fold 1/5 ---\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6818, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.08 MB) transferred to GPU in 0.005937 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2202.365232\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.009696 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2196.253063\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.013541 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.013434 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2199.880024\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.015319 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2206.654886\n[LightGBM] [Info] Start training from score 2212.253965\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.008345 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2209.309297\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007086 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2195.729841\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.004835 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Start training from score 2196.028236\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.008200 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2201.675601\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.010518 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2207.320221\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007638 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2198.547001\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\nFold 1 RMSE: 1043.9152\n[LightGBM] [Warning] Unknown parameter: silent\n--- Fold 2/5 ---\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6818, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.08 MB) transferred to GPU in 0.009778 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2189.591501\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.010452 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2197.880889\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.017541 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2182.380747\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.011989 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2191.400732\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.020272 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2188.361880\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.005245 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2194.149327\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1095\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.006011 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2179.524947\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.008638 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2181.887256\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.008231 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2195.382077\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.012673 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2191.006548\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.005769 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2193.939670\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\nFold 2 RMSE: 1096.6507\n[LightGBM] [Warning] Unknown parameter: silent\n--- Fold 3/5 ---\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6818, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.08 MB) transferred to GPU in 0.004204 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2186.145805\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007313 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2177.702895\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.013610 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2193.045483\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.014048 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2178.065417\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.017013 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2194.534852\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007722 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2187.828024\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007865 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2182.822805\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.006910 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2191.938601\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6136, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.008897 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2186.527566\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.004834 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2186.664690\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.008553 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2182.328256\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\nFold 3 RMSE: 1096.5580\n[LightGBM] [Warning] Unknown parameter: silent\n--- Fold 4/5 ---\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1101\n[LightGBM] [Info] Number of data points in the train set: 6819, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.08 MB) transferred to GPU in 0.013860 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2168.220418\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1094\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.012451 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2168.587303\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.010205 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2177.988677\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.013606 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2168.970814\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.014364 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2166.587950\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.009267 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2170.495416\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.005433 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2156.212232\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.004201 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2158.763579\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.005357 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2171.999732\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007174 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2171.233253\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6138, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.003871 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2171.364711\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\nFold 4 RMSE: 1132.0837\n[LightGBM] [Warning] Unknown parameter: silent\n--- Fold 5/5 ---\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1102\n[LightGBM] [Info] Number of data points in the train set: 6819, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.08 MB) transferred to GPU in 0.005598 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2160.126637\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] [LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\nTotal Bins 1100\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Warning] [LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\nUnknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.011353 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.011996 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2155.329996\n[LightGBM] [Info] Start training from score 2153.853454\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.016648 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.016436 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2155.536560\n[LightGBM] [Info] Start training from score 2166.752963\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007193 secs. 1 sparse feature groups\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Start training from score 2162.123390\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1102\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.038890 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2153.080791\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.008936 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2158.586850\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.004877 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2166.564842\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 6137, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.005202 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2167.150251\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 6138, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.07 MB) transferred to GPU in 0.007317 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2162.286922\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\n[LightGBM] [Warning] Unknown parameter: silent\nFold 5 RMSE: 1146.6135\n[LightGBM] [Warning] Unknown parameter: silent\n\nOverall Cross-Validation RMSE: 1103.7285\n\nPrediction complete. The output file 'submission.csv' has been generated.\n\nHead of the submission file:\n| Item_Identifier   | Outlet_Identifier   | Item_Outlet_Sales   |\n|:------------------|:--------------------|:--------------------|\n| FDW58             | OUT049              | 1687.91             |\n| FDW14             | OUT017              | 1381.3              |\n| NCN55             | OUT010              | 715.072             |\n| FDQ58             | OUT017              | 2445.76             |\n| FDY38             | OUT027              | 6053.32             |\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":" import pandas as pd\nimport numpy as np\n# UPDATED: Imported GroupKFold\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingRegressor, StackingRegressor, HistGradientBoostingRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# --- 1. Data Loading and Preprocessing ---\n\ndef load_data(train_file, test_file):\n    \"\"\"\n    Loads and combines the training and testing datasets for consistent preprocessing.\n    \"\"\"\n    try:\n        train_df = pd.read_csv(train_file)\n        test_df = pd.read_csv(test_file)\n    except FileNotFoundError as e:\n        print(f\"Error: {e}. Please ensure the files exist.\")\n        return None, None\n    \n    train_df['source'] = 'train'\n    test_df['source'] = 'test'\n    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n    \n    # Store the original Item_Identifier and Outlet_Identifier for the submission file\n    test_ids = test_df[['Item_Identifier', 'Outlet_Identifier']]\n    return combined_df, test_ids\n\ndef preprocess_features(df):\n    \"\"\"\n    Performs advanced feature engineering, cleaning, and imputation.\n    \"\"\"\n    # Impute missing Item_Weight using the median per Item_Identifier\n    df['Item_Weight'] = df.groupby('Item_Identifier')['Item_Weight'].transform(\n        lambda x: x.fillna(x.median())\n    )\n    \n    # Impute missing Outlet_Size based on the mode of the Outlet_Type\n    outlet_sizes_mode = df.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode().iloc[0])\n    df['Outlet_Size'] = df.apply(\n        lambda row: outlet_sizes_mode[row['Outlet_Type']] if pd.isna(row['Outlet_Size']) else row['Outlet_Size'],\n        axis=1\n    )\n    \n    # Handle categorical feature inconsistencies\n    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'})\n    \n    # --- Advanced Feature Engineering ---\n    \n    # 1. New feature for Outlet Age\n    df['Outlet_Years'] = 2013 - df['Outlet_Establishment_Year']\n    \n    # 2. Correcting and creating new visibility features\n    visibility_means = df.groupby('Item_Identifier')['Item_Visibility'].transform(\n        lambda x: x.replace(0, x[x != 0].mean())\n    )\n    df['Item_Visibility'] = visibility_means.fillna(df['Item_Visibility'].mean())\n    \n    # 3. Create a new feature: visibility per outlet type\n    df['Item_Visibility_per_Outlet_Type'] = df['Item_Visibility'] / df.groupby('Outlet_Type')['Item_Visibility'].transform('mean')\n\n    # 4. Item_MRP Bins\n    df['Item_MRP_Bins'] = pd.cut(df['Item_MRP'], bins=[0, 70, 140, 210, 280], labels=['Low', 'Medium', 'High', 'Very_High'])\n    \n    # 5. Item_Type Grouping\n    df['Item_Type_Combined'] = df['Item_Identifier'].apply(lambda x: x[:2])\n    df['Item_Type_Combined'] = df['Item_Type_Combined'].replace({'DR': 'Drinks', 'FD': 'Food', 'NC': 'Non-Consumables'})\n    \n    return df\n\ndef separate_and_encode(df):\n    \"\"\"\n    Separates the combined data, applies OneHotEncoder, and returns groups for CV.\n    \"\"\"\n    drop_cols = ['Outlet_Establishment_Year']\n    \n    train_processed = df[df['source'] == 'train'].drop('source', axis=1)\n    test_processed = df[df['source'] == 'test'].drop('source', axis=1)\n\n    # UPDATED: Extract the Item_Identifier to use as groups for GroupKFold\n    train_groups = train_processed['Item_Identifier']\n    \n    X_train = train_processed.drop(['Item_Outlet_Sales', 'Item_Identifier'] + drop_cols, axis=1)\n    y_train = train_processed['Item_Outlet_Sales']\n    \n    X_test = test_processed.drop(['Item_Identifier'] + drop_cols, axis=1)\n\n    categorical_cols = [\n        'Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size',\n        'Outlet_Location_Type', 'Outlet_Type', 'Item_Type_Combined', 'Item_MRP_Bins'\n    ]\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n        ],\n        remainder='passthrough'\n    )\n\n    X_train_encoded = preprocessor.fit_transform(X_train)\n    X_test_encoded = preprocessor.transform(X_test)\n    \n    # UPDATED: Return the groups along with the datasets\n    return X_train_encoded, y_train, X_test_encoded, train_groups\n\n# --- 2. Ensemble Model Building and Training ---\n\ndef build_stacked_model():\n    \"\"\"\n    Builds a Stacking Regressor model with a more diverse set of base estimators.\n    \"\"\"\n    estimators = [\n        ('xgb', xgb.XGBRegressor(\n            objective='reg:squarederror', n_estimators=100, learning_rate=0.02,\n            max_depth=2, subsample=0.7, colsample_bytree=0.7, reg_alpha=0.005, \n            random_state=42, n_jobs=-1\n        )), \n        ('lgbm', lgb.LGBMRegressor(\n            objective='regression', n_estimators=500, learning_rate=0.03,\n            num_leaves=31, min_child_samples=20, subsample=0.7,  \n            colsample_bytree=0.7, reg_alpha=0.001, random_state=42, n_jobs=-1, verbosity=-1\n        )) \n    ]\n\n    meta_regressor = LassoCV(\n        eps=1e-5, n_alphas=50, random_state=42\n    )\n    \n    stacked_model = StackingRegressor(\n        estimators=estimators,\n        final_estimator=meta_regressor,\n        cv=10,\n        n_jobs=-1\n    )\n    \n    return stacked_model\n\n# --- 3. Cross-Validation and Prediction ---\n\n# UPDATED: The function now accepts a 'groups' parameter\ndef train_and_predict(model, X_train, y_train, X_test, groups):\n    \"\"\"\n    Trains the model using GroupKFold cross-validation and makes predictions.\n    \"\"\"\n    # UPDATED: Using GroupKFold to prevent data leakage by item\n    gkf = GroupKFold(n_splits=5)\n    oof_predictions = np.zeros(X_train.shape[0])\n    test_predictions = np.zeros(X_test.shape[0])\n    \n    print(\"Starting GroupKFold Cross-Validation...\")\n    # UPDATED: The split method now uses the groups\n    for fold, (train_index, val_index) in enumerate(gkf.split(X_train, y_train, groups=groups)):\n        print(f\"--- Fold {fold+1}/{gkf.n_splits} ---\")\n        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        model.fit(X_train_fold, y_train_fold)\n        \n        oof_preds = model.predict(X_val_fold)\n        oof_predictions[val_index] = oof_preds\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, oof_preds))\n        print(f\"Fold {fold+1} RMSE: {fold_rmse:.4f}\")\n        \n        test_predictions += model.predict(X_test) / gkf.n_splits\n    \n    overall_rmse = np.sqrt(mean_squared_error(y_train, oof_predictions))\n    print(f\"\\nOverall Cross-Validation RMSE: {overall_rmse:.4f}\")\n    \n    return test_predictions\n\n# --- 4. Main Execution Block ---\n\ndef main():\n    \"\"\"\n    Main function to run the entire prediction pipeline.\n    \"\"\"\n    combined_df, test_ids = load_data('/kaggle/input/big-mart/train.csv', '/kaggle/input/big-mart/test.csv')\n\n    if combined_df is None:\n        return\n\n    combined_df = preprocess_features(combined_df.copy())\n    \n    # UPDATED: Capture the train_groups returned by the function\n    X_train_encoded, y_train, X_test_encoded, train_groups = separate_and_encode(combined_df)\n\n    print(\"Building the advanced ensemble model...\")\n    stacked_model = build_stacked_model()\n    \n    # UPDATED: Pass the train_groups to the training function\n    test_predictions = train_and_predict(stacked_model, X_train_encoded, y_train, X_test_encoded, groups=train_groups)\n    \n    submission_df = test_ids.copy()\n    submission_df['Item_Outlet_Sales'] = test_predictions\n    \n    submission_df['Item_Outlet_Sales'] = np.maximum(0, submission_df['Item_Outlet_Sales'])\n    \n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nPrediction complete. The output file 'submission.csv' has been generated.\")\n    print(\"\\nHead of the submission file:\")\n    print(submission_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n\n\nif __name__ == \"__main__\":\n    # To run this code, you must have the required data files and libraries installed.\n    # Then, you would call the main() function.\n    main()\n    print(\"Code updated successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T14:36:45.886861Z","iopub.execute_input":"2025-09-07T14:36:45.887162Z","iopub.status.idle":"2025-09-07T14:38:15.078753Z","shell.execute_reply.started":"2025-09-07T14:36:45.887139Z","shell.execute_reply":"2025-09-07T14:38:15.077839Z"}},"outputs":[{"name":"stdout","text":"Building the advanced ensemble model...\nStarting GroupKFold Cross-Validation...\n--- Fold 1/5 ---\nFold 1 RMSE: 1062.6278\n--- Fold 2/5 ---\nFold 2 RMSE: 1096.8544\n--- Fold 3/5 ---\nFold 3 RMSE: 1109.5709\n--- Fold 4/5 ---\nFold 4 RMSE: 1094.8467\n--- Fold 5/5 ---\nFold 5 RMSE: 1064.3779\n\nOverall Cross-Validation RMSE: 1085.8195\n\nPrediction complete. The output file 'submission.csv' has been generated.\n\nHead of the submission file:\n| Item_Identifier   | Outlet_Identifier   | Item_Outlet_Sales   |\n|:------------------|:--------------------|:--------------------|\n| FDW58             | OUT049              | 1550.89             |\n| FDW14             | OUT017              | 1308.67             |\n| NCN55             | OUT010              | 643.982             |\n| FDQ58             | OUT017              | 2496.02             |\n| FDY38             | OUT027              | 5896.4              |\nCode updated successfully.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\n# UPDATED: Added StandardScaler, LinearRegression, and MLPRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingRegressor, StackingRegressor, HistGradientBoostingRegressor\nfrom sklearn.linear_model import LassoCV, LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# --- 1. Data Loading and Preprocessing ---\n\ndef load_data(train_file, test_file):\n    \"\"\"\n    Loads and combines the training and testing datasets for consistent preprocessing.\n    \"\"\"\n    try:\n        train_df = pd.read_csv(train_file)\n        test_df = pd.read_csv(test_file)\n    except FileNotFoundError as e:\n        print(f\"Error: {e}. Please ensure the files exist.\")\n        return None, None\n    \n    train_df['source'] = 'train'\n    test_df['source'] = 'test'\n    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n    \n    test_ids = test_df[['Item_Identifier', 'Outlet_Identifier']]\n    return combined_df, test_ids\n\ndef feature_engineer(df):\n    \"\"\"\n    Performs advanced feature engineering, cleaning, and imputation.\n    \"\"\"\n    df['Item_Weight'] = df.groupby('Item_Identifier')['Item_Weight'].transform(\n        lambda x: x.fillna(x.median())\n    )\n    \n    outlet_sizes_mode = df.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode().iloc[0])\n    df['Outlet_Size'] = df.apply(\n        lambda row: outlet_sizes_mode[row['Outlet_Type']] if pd.isna(row['Outlet_Size']) else row['Outlet_Size'],\n        axis=1\n    )\n    \n    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'})\n    \n    df['Outlet_Years'] = 2013 - df['Outlet_Establishment_Year']\n    \n    visibility_means = df.groupby('Item_Identifier')['Item_Visibility'].transform(\n        lambda x: x.replace(0, x[x != 0].mean())\n    )\n    df['Item_Visibility'] = visibility_means.fillna(df['Item_Visibility'].mean())\n    \n    df['Item_Visibility_per_Outlet_Type'] = df['Item_Visibility'] / df.groupby('Outlet_Type')['Item_Visibility'].transform('mean')\n\n    df['Item_MRP_Bins'] = pd.cut(df['Item_MRP'], bins=[0, 70, 140, 210, 280], labels=['Low', 'Medium', 'High', 'Very_High'])\n    \n    df['Item_Type_Combined'] = df['Item_Identifier'].apply(lambda x: x[:2])\n    df['Item_Type_Combined'] = df['Item_Type_Combined'].replace({'DR': 'Drinks', 'FD': 'Food', 'NC': 'Non-Consumables'})\n    \n    return df\n\n# UPDATED: Renamed function and added StandardScaler\ndef preprocess_scale_and_encode(df):\n    \"\"\"\n    Separates data, applies scaling to numerical features and OneHotEncoder to categorical features.\n    \"\"\"\n    drop_cols = ['Outlet_Establishment_Year']\n    \n    train_processed = df[df['source'] == 'train'].drop('source', axis=1)\n    test_processed = df[df['source'] == 'test'].drop('source', axis=1)\n\n    train_groups = train_processed['Item_Identifier']\n    \n    X_train = train_processed.drop(['Item_Outlet_Sales', 'Item_Identifier'] + drop_cols, axis=1)\n    y_train = train_processed['Item_Outlet_Sales']\n    X_test = test_processed.drop(['Item_Identifier'] + drop_cols, axis=1)\n\n    # Identify categorical and numerical columns\n    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n    numerical_cols = X_train.select_dtypes(include=np.number).columns\n\n    # UPDATED: Create a preprocessor that scales numerical data and one-hot encodes categorical data\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), numerical_cols),\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n        ],\n        remainder='passthrough'\n    )\n\n    X_train_processed = preprocessor.fit_transform(X_train)\n    X_test_processed = preprocessor.transform(X_test)\n    \n    return X_train_processed, y_train, X_test_processed, train_groups\n\n# --- 2. Ensemble Model Building and Training ---\n\ndef build_stacked_model():\n    \"\"\"\n    Builds a Stacking Regressor with a diverse set of base estimators, including ML models and a DNN.\n    \"\"\"\n    estimators = [\n       \n        ('lgbm', lgb.LGBMRegressor(\n            objective='regression', n_estimators=500, learning_rate=0.03,\n            num_leaves=31, random_state=42, n_jobs=-1, verbosity=-1\n        )),\n        # NEW: Added a Deep Neural Network (Multi-layer Perceptron)\n        ('dnn', MLPRegressor(\n            hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n            alpha=0.01, max_iter=500, random_state=42, early_stopping=True\n        )),\n        # NEW: Added a Multi-Linear Regression model\n        ('lr', LinearRegression(n_jobs=-1)),\n        # ('hgbm', HistGradientBoostingRegressor(\n        #     max_iter=500, learning_rate=0.03, max_leaf_nodes=31, \n        #     min_samples_leaf=20, random_state=42\n        # ))\n    ]\n\n    meta_regressor = LassoCV(eps=1e-5, n_alphas=50, random_state=42, n_jobs=-1)\n    \n    stacked_model = StackingRegressor(\n        estimators=estimators,\n        final_estimator=meta_regressor,\n        cv=5, # Using 5 folds for the meta-model training\n        n_jobs=-1\n    )\n    \n    return stacked_model\n\n# --- 3. Cross-Validation and Prediction ---\n\ndef train_and_predict(model, X_train, y_train, X_test, groups):\n    \"\"\"\n    Trains the model using GroupKFold cross-validation and makes predictions.\n    \"\"\"\n    gkf = GroupKFold(n_splits=10)\n    oof_predictions = np.zeros(X_train.shape[0])\n    test_predictions = np.zeros(X_test.shape[0])\n    \n    print(\"Starting GroupKFold Cross-Validation...\")\n    for fold, (train_index, val_index) in enumerate(gkf.split(X_train, y_train, groups=groups)):\n        print(f\"--- Fold {fold+1}/{gkf.n_splits} ---\")\n        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        model.fit(X_train_fold, y_train_fold)\n        \n        oof_preds = model.predict(X_val_fold)\n        oof_predictions[val_index] = oof_preds\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, oof_preds))\n        print(f\"Fold {fold+1} RMSE: {fold_rmse:.4f}\")\n        \n        test_predictions += model.predict(X_test) / gkf.n_splits\n    \n    overall_rmse = np.sqrt(mean_squared_error(y_train, oof_predictions))\n    print(f\"\\nOverall Cross-Validation RMSE: {overall_rmse:.4f}\")\n    \n    return test_predictions\n\n# --- 4. Main Execution Block ---\n\ndef main():\n    \"\"\"\n    Main function to run the entire prediction pipeline.\n    \"\"\"\n    # NOTE: Update these paths to your local file locations\n    train_file = '/kaggle/input/big-mart/train.csv'\n    test_file = '/kaggle/input/big-mart/test.csv'\n\n    print(\"Loading and preprocessing data...\")\n    combined_df, test_ids = load_data(train_file, test_file)\n\n    if combined_df is None:\n        return\n\n    combined_df = feature_engineer(combined_df.copy())\n    \n    X_train_processed, y_train, X_test_processed, train_groups = preprocess_scale_and_encode(combined_df)\n\n    print(\"Building the advanced ensemble model...\")\n    stacked_model = build_stacked_model()\n    \n    test_predictions = train_and_predict(stacked_model, X_train_processed, y_train, X_test_processed, groups=train_groups)\n    \n    submission_df = test_ids.copy()\n    submission_df['Item_Outlet_Sales'] = test_predictions\n    \n    submission_df['Item_Outlet_Sales'] = np.maximum(0, submission_df['Item_Outlet_Sales'])\n    \n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nPrediction complete. The output file 'submission.csv' has been generated.\")\n    print(\"\\nHead of the submission file:\")\n    print(submission_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n\n\nif __name__ == \"__main__\":\n    # To run this code, you must have the required data files (train.csv, test.csv)\n    # in the same directory and necessary libraries (sklearn, pandas, xgboost, lightgbm) installed.\n    # Then, you would call the main() function.\n    main()\n    print(\"Code updated successfully with scaling, a DNN, and Linear Regression.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T15:05:13.791515Z","iopub.execute_input":"2025-09-07T15:05:13.791889Z","iopub.status.idle":"2025-09-07T15:12:19.631278Z","shell.execute_reply.started":"2025-09-07T15:05:13.791866Z","shell.execute_reply":"2025-09-07T15:12:19.630503Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing data...\nBuilding the advanced ensemble model...\nStarting GroupKFold Cross-Validation...\n--- Fold 1/10 ---\nFold 1 RMSE: 1080.5470\n--- Fold 2/10 ---\nFold 2 RMSE: 1082.8000\n--- Fold 3/10 ---\nFold 3 RMSE: 1140.3315\n--- Fold 4/10 ---\nFold 4 RMSE: 1038.8970\n--- Fold 5/10 ---\nFold 5 RMSE: 1059.6448\n--- Fold 6/10 ---\nFold 6 RMSE: 1020.6587\n--- Fold 7/10 ---\nFold 7 RMSE: 1102.5391\n--- Fold 8/10 ---\nFold 8 RMSE: 1065.4548\n--- Fold 9/10 ---\nFold 9 RMSE: 1145.7432\n--- Fold 10/10 ---\nFold 10 RMSE: 1042.5348\n\nOverall Cross-Validation RMSE: 1078.6501\n\nPrediction complete. The output file 'submission.csv' has been generated.\n\nHead of the submission file:\n| Item_Identifier   | Outlet_Identifier   | Item_Outlet_Sales   |\n|:------------------|:--------------------|:--------------------|\n| FDW58             | OUT049              | 1740.68             |\n| FDW14             | OUT017              | 1476.15             |\n| NCN55             | OUT010              | 643.988             |\n| FDQ58             | OUT017              | 2615.84             |\n| FDY38             | OUT027              | 5709.05             |\nCode updated successfully with scaling, a DNN, and Linear Regression.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"1","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}