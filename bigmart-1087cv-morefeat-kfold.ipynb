{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:17:18.280515Z","iopub.execute_input":"2025-09-07T13:17:18.280832Z","iopub.status.idle":"2025-09-07T13:17:21.635231Z","shell.execute_reply.started":"2025-09-07T13:17:18.280810Z","shell.execute_reply":"2025-09-07T13:17:21.634360Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor, HistGradientBoostingRegressor\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# --- 1. Data Loading and Preprocessing ---\n\ndef load_data(train_file, test_file):\n    \"\"\"\n    Loads and combines the training and testing datasets for consistent preprocessing.\n    \"\"\"\n    try:\n        train_df = pd.read_csv(train_file)\n        test_df = pd.read_csv(test_file)\n    except FileNotFoundError as e:\n        print(f\"Error: {e}. Please ensure the files exist.\")\n        return None, None\n    \n    train_df['source'] = 'train'\n    test_df['source'] = 'test'\n    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n    \n    # Store the original Item_Identifier and Outlet_Identifier for the submission file\n    test_ids = test_df[['Item_Identifier', 'Outlet_Identifier']]\n    return combined_df, test_ids\n\ndef preprocess_features(df):\n    \"\"\"\n    Performs advanced feature engineering, cleaning, and imputation.\n    \"\"\"\n    # Impute missing Item_Weight using the median per Item_Identifier\n    # Using median is more robust to outliers than mean.\n    df['Item_Weight'] = df.groupby('Item_Identifier')['Item_Weight'].transform(\n        lambda x: x.fillna(x.median())\n    )\n    \n    # Impute missing Outlet_Size based on the mode of the Outlet_Type\n    outlet_sizes_mode = df.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode().iloc[0])\n    df['Outlet_Size'] = df.apply(\n        lambda row: outlet_sizes_mode[row['Outlet_Type']] if pd.isna(row['Outlet_Size']) else row['Outlet_Size'],\n        axis=1\n    )\n    \n    # Handle categorical feature inconsistencies\n    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'})\n    \n    # --- Advanced Feature Engineering ---\n    \n    # 1. New feature for Outlet Age\n    df['Outlet_Years'] = 2013 - df['Outlet_Establishment_Year']\n    \n    # 2. Correcting and creating new visibility features\n    # First, replace 0 visibility with the mean visibility of that item across all outlets\n    visibility_means = df.groupby('Item_Identifier')['Item_Visibility'].transform(\n        lambda x: x.replace(0, x[x != 0].mean())\n    )\n    # If an item had 0 visibility across all its appearances, fill with the global average visibility\n    df['Item_Visibility'] = visibility_means.fillna(df['Item_Visibility'].mean())\n    \n    # Create a new feature: visibility per outlet type\n    df['Item_Visibility_per_Outlet_Type'] = df['Item_Visibility'] / df.groupby('Outlet_Type')['Item_Visibility'].transform('mean')\n\n    # 3. Item_MRP Bins\n    # Binning the Item_MRP into categories can help the model capture non-linear relationships.\n    df['Item_MRP_Bins'] = pd.cut(df['Item_MRP'], bins=[0, 70, 140, 210, 280], labels=['Low', 'Medium', 'High', 'Very_High'])\n    \n    # 4. Item_Type Grouping\n    df['Item_Type_Combined'] = df['Item_Identifier'].apply(lambda x: x[:2])\n    df['Item_Type_Combined'] = df['Item_Type_Combined'].replace({'DR': 'Drinks', 'FD': 'Food', 'NC': 'Non-Consumables'})\n    \n    return df\n\ndef separate_and_encode(df):\n    \"\"\"\n    Separates the combined data and applies a single OneHotEncoder.\n    \"\"\"\n    # Define features to be dropped before encoding\n    drop_cols = ['Item_Identifier', 'Outlet_Establishment_Year']\n    \n    # Separate data back into train and test sets\n    train_processed = df[df['source'] == 'train'].drop('source', axis=1)\n    test_processed = df[df['source'] == 'test'].drop('source', axis=1)\n\n    X_train = train_processed.drop(['Item_Outlet_Sales'] + drop_cols, axis=1)\n    y_train = train_processed['Item_Outlet_Sales']\n    \n    X_test = test_processed.drop(drop_cols, axis=1)\n\n    # Use OneHotEncoder within a ColumnTransformer\n    categorical_cols = [\n        'Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size',\n        'Outlet_Location_Type', 'Outlet_Type', 'Item_Type_Combined', 'Item_MRP_Bins'\n    ]\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n        ],\n        remainder='passthrough'\n    )\n\n    X_train_encoded = preprocessor.fit_transform(X_train)\n    X_test_encoded = preprocessor.transform(X_test)\n    \n    return X_train_encoded, y_train, X_test_encoded\n\n# --- 2. Ensemble Model Building and Training ---\n\ndef build_stacked_model():\n    \"\"\"\n    Builds a Stacking Regressor model with a more diverse set of base estimators.\n    \"\"\"\n    # Optimized hyperparameters for the base models to reduce overfitting\n    # Added 'tree_method' for XGBoost and 'device' for LightGBM to enable GPU usage.\n    estimators = [\n        ('xgb', xgb.XGBRegressor(\n            objective='reg:squarederror', n_estimators=100, learning_rate=0.03,\n            max_depth=4, subsample=0.7, colsample_bytree=0.7, reg_alpha=0.005, \n            random_state=42, n_jobs=-1, device='cuda'\n        )),\n        ('gbr', GradientBoostingRegressor(\n            n_estimators=500, learning_rate=0.03, max_depth=3,\n            min_samples_leaf=20, max_features='sqrt', random_state=42\n        )),\n        ('lgbm', lgb.LGBMRegressor(\n            objective='regression', n_estimators=500, learning_rate=0.03,\n            num_leaves=31, min_child_samples=20, subsample=0.7, \n            colsample_bytree=0.7, reg_alpha=0.001, random_state=42, n_jobs=-1,\n            device='gpu' # Enables GPU for faster training\n        )),\n        ('hgbm', HistGradientBoostingRegressor(\n            max_iter=500, learning_rate=0.03, max_leaf_nodes=31, \n            min_samples_leaf=20, random_state=42\n        ))\n    ]\n\n    # Use a more flexible meta-regressor like LassoCV\n    # The 'normalize' parameter was removed in recent scikit-learn versions, so it's been removed here.\n    meta_regressor = LassoCV(\n        eps=1e-5, n_alphas=200, random_state=42\n    )\n    \n    # Create the StackingRegressor with 10-fold cross-validation\n    stacked_model = StackingRegressor(\n        estimators=estimators,\n        final_estimator=meta_regressor,\n        cv=5, # Increased folds for more robust cross-validation\n        n_jobs=-1\n    )\n    \n    return stacked_model\n\n# --- 3. Cross-Validation and Prediction ---\n\ndef train_and_predict(model, X_train, y_train, X_test):\n    \"\"\"\n    Trains the model using K-Fold cross-validation and makes predictions.\n    \"\"\"\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_predictions = np.zeros(X_train.shape[0])\n    test_predictions = np.zeros(X_test.shape[0])\n    \n    print(\"Starting K-Fold Cross-Validation...\")\n    for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold+1}/{kf.n_splits} ---\")\n        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        model.fit(X_train_fold, y_train_fold)\n        \n        oof_preds = model.predict(X_val_fold)\n        oof_predictions[val_index] = oof_preds\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, oof_preds))\n        print(f\"Fold {fold+1} RMSE: {fold_rmse:.4f}\")\n        \n        test_predictions += model.predict(X_test) / kf.n_splits\n    \n    overall_rmse = np.sqrt(mean_squared_error(y_train, oof_predictions))\n    print(f\"\\nOverall Cross-Validation RMSE: {overall_rmse:.4f}\")\n    \n    return test_predictions\n\n# --- 4. Main Execution Block ---\n\ndef main():\n    \"\"\"\n    Main function to run the entire prediction pipeline.\n    \"\"\"\n    print(\"Loading and preprocessing data...\")\n    combined_df, test_ids = load_data('/kaggle/input/big-mart/train.csv', '/kaggle/input/big-mart/test.csv')\n\n    if combined_df is None:\n        return\n\n    combined_df = preprocess_features(combined_df.copy())\n    \n    X_train_encoded, y_train, X_test_encoded = separate_and_encode(combined_df)\n\n    print(\"Building the advanced ensemble model...\")\n    stacked_model = build_stacked_model()\n    \n    test_predictions = train_and_predict(stacked_model, X_train_encoded, y_train, X_test_encoded)\n    \n    # Create the submission DataFrame\n    submission_df = test_ids.copy()\n    submission_df['Item_Outlet_Sales'] = test_predictions\n    \n    # Post-process predictions to be non-negative\n    submission_df['Item_Outlet_Sales'] = np.maximum(0, submission_df['Item_Outlet_Sales'])\n    \n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nPrediction complete. The output file 'submission.csv' has been generated.\")\n    print(\"\\nHead of the submission file:\")\n    print(submission_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:27:46.912623Z","iopub.execute_input":"2025-09-07T13:27:46.913540Z","iopub.status.idle":"2025-09-07T13:30:35.393565Z","shell.execute_reply.started":"2025-09-07T13:27:46.913511Z","shell.execute_reply":"2025-09-07T13:30:35.392825Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing data...\nBuilding the advanced ensemble model...\nStarting K-Fold Cross-Validation...\n--- Fold 1/5 ---\n","output_type":"stream"},{"name":"stderr","text":"[13:27:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1095\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.016034 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.007216 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2218.316567\n[LightGBM] [Info] Start training from score 2202.712505\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.009084 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2192.692789\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.009283 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2194.279807\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.004439 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2203.825707\nFold 1 RMSE: 1017.0308\n--- Fold 2/5 ---\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.008554 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2180.095693\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.011162 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.016631 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.009073 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2183.393929\n[LightGBM] [Info] Start training from score 2200.952909\n[LightGBM] [Info] Start training from score 2187.144457\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.004974 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2196.369724\nFold 2 RMSE: 1077.2362\n--- Fold 3/5 ---\n","output_type":"stream"},{"name":"stderr","text":"[13:29:05] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1101\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 5454, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.018349 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.018083 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2192.979452\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.028433 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2184.299851\n[LightGBM] [Info] Start training from score 2167.556337\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.013144 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2203.346321\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1101\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.006227 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2182.546471\nFold 3 RMSE: 1068.1800\n--- Fold 4/5 ---\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1097\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.008784 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2169.477384\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.011447 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.013822 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Start training from score 2177.373370\n[LightGBM] [Info] Start training from score 2161.833072\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.024318 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2157.270359\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 5456, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.006730 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2175.146636\nFold 4 RMSE: 1113.7515\n--- Fold 5/5 ---\n","output_type":"stream"},{"name":"stderr","text":"[13:30:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Total Bins 1096\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1100\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 5455, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.009791 secs. 1 sparse feature groups\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Start training from score 2154.446292\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.012394 secs. 1 sparse feature groups\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.008576 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2165.637469\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.008200 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2147.905221\n[LightGBM] [Info] Start training from score 2162.185074\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 1099\n[LightGBM] [Info] Number of data points in the train set: 5456, number of used features: 50\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 12 dense feature groups (0.06 MB) transferred to GPU in 0.004513 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2170.457236\nFold 5 RMSE: 1115.2400\n\nOverall Cross-Validation RMSE: 1078.8799\n\nPrediction complete. The output file 'submission.csv' has been generated.\n\nHead of the submission file:\n| Item_Identifier   | Outlet_Identifier   | Item_Outlet_Sales   |\n|:------------------|:--------------------|:--------------------|\n| FDW58             | OUT049              | 1675.81             |\n| FDW14             | OUT017              | 1380.82             |\n| NCN55             | OUT010              | 618.593             |\n| FDQ58             | OUT017              | 2541.6              |\n| FDY38             | OUT027              | 6083.68             |\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}