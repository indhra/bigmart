{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:38:04.649191Z","iopub.execute_input":"2025-09-07T13:38:04.649481Z","iopub.status.idle":"2025-09-07T13:38:07.900546Z","shell.execute_reply.started":"2025-09-07T13:38:04.649461Z","shell.execute_reply":"2025-09-07T13:38:07.899486Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor, HistGradientBoostingRegressor\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# --- 1. Data Loading and Preprocessing ---\n\ndef load_data(train_file, test_file):\n    \"\"\"\n    Loads and combines the training and testing datasets for consistent preprocessing.\n    \"\"\"\n    try:\n        train_df = pd.read_csv(train_file)\n        test_df = pd.read_csv(test_file)\n    except FileNotFoundError as e:\n        print(f\"Error: {e}. Please ensure the files exist.\")\n        return None, None\n    \n    train_df['source'] = 'train'\n    test_df['source'] = 'test'\n    combined_df = pd.concat([train_df, test_df], ignore_index=True, sort=False)\n    \n    # Store the original Item_Identifier and Outlet_Identifier for the submission file\n    test_ids = test_df[['Item_Identifier', 'Outlet_Identifier']]\n    return combined_df, test_ids\n\ndef preprocess_features(df):\n    \"\"\"\n    Performs advanced feature engineering, cleaning, and imputation.\n    \"\"\"\n    # Impute missing Item_Weight using the median per Item_Identifier\n    # Using median is more robust to outliers than mean.\n    df['Item_Weight'] = df.groupby('Item_Identifier')['Item_Weight'].transform(\n        lambda x: x.fillna(x.median())\n    )\n    \n    # Impute missing Outlet_Size based on the mode of the Outlet_Type\n    outlet_sizes_mode = df.groupby('Outlet_Type')['Outlet_Size'].apply(lambda x: x.mode().iloc[0])\n    df['Outlet_Size'] = df.apply(\n        lambda row: outlet_sizes_mode[row['Outlet_Type']] if pd.isna(row['Outlet_Size']) else row['Outlet_Size'],\n        axis=1\n    )\n    \n    # Handle categorical feature inconsistencies\n    df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'low fat':'Low Fat', 'LF':'Low Fat', 'reg':'Regular'})\n    \n    # --- Advanced Feature Engineering ---\n    \n    # 1. New feature for Outlet Age\n    df['Outlet_Years'] = 2013 - df['Outlet_Establishment_Year']\n    \n    # 2. Correcting and creating new visibility features\n    # First, replace 0 visibility with the mean visibility of that item across all outlets\n    visibility_means = df.groupby('Item_Identifier')['Item_Visibility'].transform(\n        lambda x: x.replace(0, x[x != 0].mean())\n    )\n    # If an item had 0 visibility across all its appearances, fill with the global average visibility\n    df['Item_Visibility'] = visibility_means.fillna(df['Item_Visibility'].mean())\n    \n    # Create a new feature: visibility per outlet type\n    df['Item_Visibility_per_Outlet_Type'] = df['Item_Visibility'] / df.groupby('Outlet_Type')['Item_Visibility'].transform('mean')\n\n    # 3. Item_MRP Bins\n    # Binning the Item_MRP into categories can help the model capture non-linear relationships.\n    df['Item_MRP_Bins'] = pd.cut(df['Item_MRP'], bins=[0, 70, 140, 210, 280], labels=['Low', 'Medium', 'High', 'Very_High'])\n    \n    # 4. Item_Type Grouping\n    df['Item_Type_Combined'] = df['Item_Identifier'].apply(lambda x: x[:2])\n    df['Item_Type_Combined'] = df['Item_Type_Combined'].replace({'DR': 'Drinks', 'FD': 'Food', 'NC': 'Non-Consumables'})\n    \n    return df\n\ndef separate_and_encode(df):\n    \"\"\"\n    Separates the combined data and applies a single OneHotEncoder.\n    \"\"\"\n    # Define features to be dropped before encoding\n    drop_cols = ['Item_Identifier', 'Outlet_Establishment_Year']\n    \n    # Separate data back into train and test sets\n    train_processed = df[df['source'] == 'train'].drop('source', axis=1)\n    test_processed = df[df['source'] == 'test'].drop('source', axis=1)\n\n    X_train = train_processed.drop(['Item_Outlet_Sales'] + drop_cols, axis=1)\n    y_train = train_processed['Item_Outlet_Sales']\n    \n    X_test = test_processed.drop(drop_cols, axis=1)\n\n    # Use OneHotEncoder within a ColumnTransformer\n    categorical_cols = [\n        'Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size',\n        'Outlet_Location_Type', 'Outlet_Type', 'Item_Type_Combined', 'Item_MRP_Bins'\n    ]\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n        ],\n        remainder='passthrough'\n    )\n\n    X_train_encoded = preprocessor.fit_transform(X_train)\n    X_test_encoded = preprocessor.transform(X_test)\n    \n    return X_train_encoded, y_train, X_test_encoded\n\n# --- 2. Ensemble Model Building and Training ---\n\ndef build_stacked_model():\n    \"\"\"\n    Builds a Stacking Regressor model with a more diverse set of base estimators.\n    \"\"\"\n    # Optimized hyperparameters for the base models to reduce overfitting\n    # Added 'tree_method' for XGBoost and 'device' for LightGBM to enable GPU usage.\n    estimators = [\n        ('xgb', xgb.XGBRegressor(\n            objective='reg:squarederror', n_estimators=100, learning_rate=0.03,\n            max_depth=5, subsample=0.7, colsample_bytree=0.7, reg_alpha=0.005, \n            random_state=42, n_jobs=-1, device='cuda'\n        )),\n        ('gbr', GradientBoostingRegressor(\n            n_estimators=500, learning_rate=0.03, max_depth=5,\n            min_samples_leaf=20, max_features='sqrt', random_state=42\n        )),\n        # ('lgbm', lgb.LGBMRegressor(\n        #     objective='regression', n_estimators=500, learning_rate=0.03,\n        #     num_leaves=31, min_child_samples=20, subsample=0.7,  max_depth=5,\n        #     colsample_bytree=0.7, reg_alpha=0.001, random_state=42, n_jobs=-1, silent=True,\n        #     device='gpu' # Enables GPU for faster training\n        # )),\n        ('hgbm', HistGradientBoostingRegressor(\n            max_iter=500, learning_rate=0.03, max_leaf_nodes=31, \n            min_samples_leaf=20, random_state=42\n        ))\n    ]\n\n    # Use a more flexible meta-regressor like LassoCV\n    # The 'normalize' parameter was removed in recent scikit-learn versions, so it's been removed here.\n    meta_regressor = LassoCV(\n        eps=1e-5, n_alphas=50, random_state=42\n    )\n    \n    # Create the StackingRegressor with 10-fold cross-validation\n    stacked_model = StackingRegressor(\n        estimators=estimators,\n        final_estimator=meta_regressor,\n        cv=5, # Increased folds for more robust cross-validation\n        n_jobs=-1\n    )\n    \n    return stacked_model\n\n# --- 3. Cross-Validation and Prediction ---\n\ndef train_and_predict(model, X_train, y_train, X_test):\n    \"\"\"\n    Trains the model using K-Fold cross-validation and makes predictions.\n    \"\"\"\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_predictions = np.zeros(X_train.shape[0])\n    test_predictions = np.zeros(X_test.shape[0])\n    \n    print(\"Starting K-Fold Cross-Validation...\")\n    for fold, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n        print(f\"--- Fold {fold+1}/{kf.n_splits} ---\")\n        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n        \n        model.fit(X_train_fold, y_train_fold)\n        \n        oof_preds = model.predict(X_val_fold)\n        oof_predictions[val_index] = oof_preds\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, oof_preds))\n        print(f\"Fold {fold+1} RMSE: {fold_rmse:.4f}\")\n        \n        test_predictions += model.predict(X_test) / kf.n_splits\n    \n    overall_rmse = np.sqrt(mean_squared_error(y_train, oof_predictions))\n    print(f\"\\nOverall Cross-Validation RMSE: {overall_rmse:.4f}\")\n    \n    return test_predictions\n\n# --- 4. Main Execution Block ---\n\ndef main():\n    \"\"\"\n    Main function to run the entire prediction pipeline.\n    \"\"\"\n    print(\"Loading and preprocessing data...\")\n    combined_df, test_ids = load_data('/kaggle/input/big-mart/train.csv', '/kaggle/input/big-mart/test.csv')\n\n    if combined_df is None:\n        return\n\n    combined_df = preprocess_features(combined_df.copy())\n    \n    X_train_encoded, y_train, X_test_encoded = separate_and_encode(combined_df)\n\n    print(\"Building the advanced ensemble model...\")\n    stacked_model = build_stacked_model()\n    \n    test_predictions = train_and_predict(stacked_model, X_train_encoded, y_train, X_test_encoded)\n    \n    # Create the submission DataFrame\n    submission_df = test_ids.copy()\n    submission_df['Item_Outlet_Sales'] = test_predictions\n    \n    # Post-process predictions to be non-negative\n    submission_df['Item_Outlet_Sales'] = np.maximum(0, submission_df['Item_Outlet_Sales'])\n    \n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nPrediction complete. The output file 'submission.csv' has been generated.\")\n    print(\"\\nHead of the submission file:\")\n    print(submission_df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:38:07.902083Z","iopub.execute_input":"2025-09-07T13:38:07.902415Z","iopub.status.idle":"2025-09-07T13:39:57.433436Z","shell.execute_reply.started":"2025-09-07T13:38:07.902377Z","shell.execute_reply":"2025-09-07T13:39:57.432754Z"}},"outputs":[{"name":"stdout","text":"Loading and preprocessing data...\nBuilding the advanced ensemble model...\nStarting K-Fold Cross-Validation...\n--- Fold 1/5 ---\n","output_type":"stream"},{"name":"stderr","text":"[13:38:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 RMSE: 1020.7796\n--- Fold 2/5 ---\n","output_type":"stream"},{"name":"stderr","text":"[13:38:37] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"},{"name":"stdout","text":"Fold 2 RMSE: 1075.9766\n--- Fold 3/5 ---\nFold 3 RMSE: 1073.3187\n--- Fold 4/5 ---\n","output_type":"stream"},{"name":"stderr","text":"[13:39:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 RMSE: 1117.3147\n--- Fold 5/5 ---\nFold 5 RMSE: 1116.1154\n\nOverall Cross-Validation RMSE: 1081.2718\n\nPrediction complete. The output file 'submission.csv' has been generated.\n\nHead of the submission file:\n| Item_Identifier   | Outlet_Identifier   | Item_Outlet_Sales   |\n|:------------------|:--------------------|:--------------------|\n| FDW58             | OUT049              | 1668.2              |\n| FDW14             | OUT017              | 1373.99             |\n| NCN55             | OUT010              | 589.715             |\n| FDQ58             | OUT017              | 2523.8              |\n| FDY38             | OUT027              | 6174.25             |\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}